{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import string\n",
    "from collections import Counter\n",
    "import xmltodict\n",
    "import rnnmorph\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from russian_tagsets import converters\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Bidirectional, Dropout, Dense, Input, Embedding, BatchNormalization, \\\n",
    "    TimeDistributed, GRU, Conv1D, MaxPooling1D, Flatten, Reshape, Conv2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "from tokenizer import Tokenizer\n",
    "from vectorizer import GrammemeVectorizer\n",
    "\n",
    "morph_ru = MorphAnalyzer()\n",
    "to_ud = converters.converter('opencorpora-int', 'ud14')\n",
    "nltk.download('stopwords')\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = lambda x : [token.text for token in Tokenizer.tokenize(x) \\\n",
    "                       if token.token_type == Token.TokenType.WORD or token.token_type == Token.TokenType.PUNCTUATION]\n",
    "tokenize_lower = lambda x : [token.lower() for token in tokenize(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(sentence):\n",
    "    regexp = \"[^а-яА-Яёa-zA-Z]\"\n",
    "    sentence = re.sub(regexp, \" \", sentence)\n",
    "    result = sentence.lower().split()\n",
    "    return result\n",
    "\n",
    "\n",
    "def stem_sentence(sentence, language):\n",
    "    words = tokenize(sentence)\n",
    "    for j in range(len(words)):\n",
    "        if language == 'ru':\n",
    "            words[j] = morph_ru.parse(words[j])[0].normal_form\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def get_sentence_tags(sentence):\n",
    "    words = tokenize(sentence)\n",
    "    tags = []\n",
    "    for j in range(len(words)):\n",
    "        pos = morph_ru.parse(words[j])[0].tag.POS\n",
    "        if pos is not None:\n",
    "            tags.append(pos)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def bow(train_texts, test_texts, language='ru', stem=False, tokenizer=tokenize_lower, preprocessor=None,\n",
    "        use_tfidf=False, max_features=None, bow_ngrams=(1,1), analyzer='word'):\n",
    "    train = copy.deepcopy(train_texts)\n",
    "    test = copy.deepcopy(test_texts)\n",
    "    if stem:\n",
    "        for i in range(len(train)):\n",
    "            train[i] = stem_sentence(train[i], language)\n",
    "        for i in range(len(test)):\n",
    "            test[i] = stem_sentence(test[i], language)\n",
    "\n",
    "    if use_tfidf:\n",
    "        vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n",
    "                                     preprocessor=preprocessor, max_features=max_features)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n",
    "                                     preprocessor=preprocessor, max_features=max_features)\n",
    "    data = train+test\n",
    "    data = vectorizer.fit_transform(data)\n",
    "    train_data = data[:len(train)]\n",
    "    test_data = data[len(train):]\n",
    "    return train_data, test_data\n",
    "\n",
    "def convert_from_opencorpora_tag(to_ud, tag: str, text: str):\n",
    "    \"\"\"\n",
    "    Конвертировать теги их формата OpenCorpora в Universal Dependencies\n",
    "    \n",
    "    :param to_ud: конвертер.\n",
    "    :param tag: тег в OpenCorpora.\n",
    "    :param text: токен.\n",
    "    :return: тег в UD.\n",
    "    \"\"\"\n",
    "    ud_tag = to_ud(str(tag), text)\n",
    "    pos = ud_tag.split()[0]\n",
    "    gram = ud_tag.split()[1]\n",
    "    return pos, gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = {}\n",
    "def get_sample_text(sample):\n",
    "    assert sample['column'][3]['@name'] == 'text'\n",
    "    return sample['column'][3]['#text']\n",
    "\n",
    "\n",
    "def get_sample_answers(sample):\n",
    "    answers = {}\n",
    "    for i in range(4, 12):\n",
    "        companies[sample['column'][i]['@name']] = i\n",
    "        answers[sample['column'][i]['@name']] = None if sample['column'][i]['#text'] == 'NULL'\\\n",
    "            else int(sample['column'][i]['#text'])\n",
    "    return answers\n",
    "\n",
    "\n",
    "def get_sample_id(sample):\n",
    "    assert sample['column'][0]['@name'] == 'id'\n",
    "    return int(sample['column'][0]['#text'])\n",
    "\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.DataFrame()\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        d = xmltodict.parse(f.read(), process_namespaces=True)\n",
    "        clean_samples = []\n",
    "        for sample in d['pma_xml_export']['database']['table']:\n",
    "            sample_id = get_sample_id(sample)\n",
    "            text = get_sample_text(sample)\n",
    "            answers = get_sample_answers(sample)\n",
    "            for company, answer in answers.items():\n",
    "                if answer is not None:\n",
    "                    clean_samples.append((sample_id, text, company, answer))\n",
    "        df['text'] = [sample[1] for sample in clean_samples]\n",
    "        df['answer'] = [sample[3] for sample in clean_samples]\n",
    "        df['company'] = [sample[2] for sample in clean_samples]\n",
    "        df['sample_id'] = [sample[0] for sample in clean_samples]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = \"/data/banks_train.xml\"\n",
    "test_filename = \"/data/banks_test.xml\"\n",
    "\n",
    "train = get_data(train_filename)\n",
    "test = get_data(test_filename)\n",
    "\n",
    "url_replacement = lambda x: re.sub(r'(?:http[^\\s]+)($|\\s)', r'url\\1', x)\n",
    "user_replacement = lambda x: re.sub(r'(?:@[^\\s]+)($|\\s)', r'user\\1', x)\n",
    "\n",
    "train['text'] = train['text'].apply(url_replacement)\n",
    "train['text'] = train['text'].apply(user_replacement)\n",
    "\n",
    "test['text'] = test['text'].apply(url_replacement)\n",
    "test['text'] = test['text'].apply(user_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'tol': [0.0001, 0.001]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score=True, scoring='accuracy', verbose=0)\n",
      "0.821445221445\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "                                                text  answer company  \\\n",
      "0              #Автокредит в россельхозбанк 2012 url       0    rshb   \n",
      "1           #Автокредит в россельхозбанк в череповце       0    rshb   \n",
      "2  RT user url #Кредитный калькулятор россельхозб...       0    rshb   \n",
      "3        RT user #Кредитные карты россельхозбанк url       0    rshb   \n",
      "4      RT user #Кредиты в россельхозбанке ижевск url       0    rshb   \n",
      "\n",
      "   sample_id  \n",
      "0          1  \n",
      "1          2  \n",
      "2          3  \n",
      "3          4  \n",
      "4          5  \n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = bow(train['text'].tolist(), test['text'].tolist(), stem=True, bow_ngrams=(1,2))\n",
    "boc_train_data, boc_test_data = bow(train['text'].tolist(), test['text'].tolist(), analyzer='char', tokenizer=None, bow_ngrams=(1,2))\n",
    "train_data = hstack([train_data, boc_train_data])\n",
    "test_data = hstack([test_data, boc_test_data])\n",
    "\n",
    "train_answer = train['answer'].tolist()\n",
    "\n",
    "clf = GridSearchCV(estimator=LogisticRegression(class_weight='balanced'),\n",
    "                   param_grid={\"tol\": [1e-4, 1e-3],},\n",
    "                   scoring=\"accuracy\", cv=5)\n",
    "clf.fit(train_data, train_answer)\n",
    "print(clf)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "test['answer'] = clf.predict(test_data)\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(filename, test_df, output_filename):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        d = xmltodict.parse(f.read(), process_namespaces=True)\n",
    "        test_answers = test_df['answer'].tolist()\n",
    "        test_sample_ids = test_df['sample_id'].tolist()\n",
    "        j = 0\n",
    "        del d['pma_xml_export']['http://www.phpmyadmin.net/some_doc_url/:structure_schemas']\n",
    "        for i, sample in enumerate(d['pma_xml_export']['database']['table']):\n",
    "            sample_id = get_sample_id(sample)\n",
    "            answers = get_sample_answers(sample)\n",
    "            for company, answer in answers.items():\n",
    "                if answer is not None:\n",
    "                    assert sample_id == test_sample_ids[j]\n",
    "                    sample['column'][companies[company]]['#text'] = str(test_answers[j])\n",
    "                    j += 1\n",
    "        with open(output_filename, \"w\", encoding='utf-8') as w:\n",
    "            xmltodict.unparse(d, w, pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_submission(test_filename, test, \"submission.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts    -  positive { tp: 145, tn: 2917, fp: 183, fn: 173 } negative { tp: 412, tn: 2304, fp: 330, fn: 372 }\r\n",
      "Precision -  { positive: 0.4420731707317073, negative: 0.555256064690027 }\r\n",
      "Recall    -  { positive: 0.4559748427672956, negative: 0.5255102040816326 }\r\n",
      "F         -  { positive: 0.4489164086687306, negative: 0.5399737876802096 }\r\n",
      "F_R       -  0.49444509817447013\r\n"
     ]
    }
   ],
   "source": [
    "!cd twit-calc-win64/; nodejs calc.js bank ../submission.xml ../etalon.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21864\n"
     ]
    }
   ],
   "source": [
    "VOCAB_PATH = \"vocab.pickle\"\n",
    "def prepare_vocabulary(vocab_path, train, test, shrink_border=None):\n",
    "    vocabulary = Vocabulary(vocab_path)\n",
    "    if vocabulary.size() <= 1:\n",
    "        for sentence in train['text'].tolist():\n",
    "            for word in tokenize_lower(sentence):\n",
    "                vocabulary.add_word(word)\n",
    "        print(\"Train vocabulary size: {}\".format(vocabulary.size()))\n",
    "        for sentence in test['text'].tolist():\n",
    "            for word in tokenize_lower(sentence):\n",
    "                vocabulary.add_word(word) \n",
    "        print(\"Train+test vocabulary size: {}\".format(vocabulary.size()))\n",
    "        vocabulary.save()\n",
    "\n",
    "    print(\"Vocabulary size: {}\".format(vocabulary.size()))\n",
    "    if shrink_border is not None:\n",
    "        vocabulary.shrink(shrink_border)\n",
    "        print(\"Vocabulary size after shrink: {}\".format(vocabulary.size()))\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = prepare_vocabulary(VOCAB_PATH, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"web_0_300_20.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words:  5307\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.uniform(-0.01, 0.01, size=(vocabulary.size() + 1, 300))\n",
    "unknown_words_count = 0\n",
    "for i, word in enumerate(vocabulary.index_to_word):\n",
    "    parse = morph_ru.parse(word)[0]\n",
    "    pos = parse.tag.POS\n",
    "    if pos is None:\n",
    "        continue\n",
    "    if pos == 'ADJF' or pos == 'ADJS' or pos == 'COMP':\n",
    "        pos ='ADJ'\n",
    "    if pos == 'INFN':\n",
    "        pos = 'VERB'\n",
    "    if pos == 'ADVB':\n",
    "        pos = 'ADV'\n",
    "    lemma = parse.normal_form + '_' + pos\n",
    "    if lemma in model.wv:\n",
    "        weights[i] = model.wv[lemma]\n",
    "    else:\n",
    "        unknown_words_count += 1\n",
    "weights[0] = np.zeros((300,))\n",
    "print(\"Unknown words: \", unknown_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "grammeme_vectorizer = GrammemeVectorizer(\"vectorizer.json\")\n",
    "predictor = RNNMorphPredictor()\n",
    "\n",
    "if grammeme_vectorizer.size() < 1:\n",
    "    for i, sentence in enumerate(train['text'].tolist()):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        forms = predictor.predict_sentence_tags(tokenize(sentence))\n",
    "        for form in forms:\n",
    "            grammeme_vectorizer.add_grammemes(form.pos, form.tag)\n",
    "    for i, sentence in enumerate(test['text'].tolist()):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        forms = predictor.predict_sentence_tags(tokenize(sentence))\n",
    "        for form in forms:\n",
    "            grammeme_vectorizer.add_grammemes(form.pos, form.tag)\n",
    "    grammeme_vectorizer.init_possible_vectors()\n",
    "    grammeme_vectorizer.save()\n",
    "print(grammeme_vectorizer.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SET = \" абвгдеёжзийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-'\\\"\"\n",
    "\n",
    "def get_samples(sentences, vocabulary, word_max_count, max_word_len):\n",
    "    n = len(sentences)\n",
    "    word_matrix = np.zeros((n, word_max_count), dtype='int')\n",
    "    char_matrix = np.zeros((n, word_max_count, max_word_len), dtype=np.int)\n",
    "    grammemes_matrix = np.zeros((n, word_max_count, grammeme_vectorizer.grammemes_count()), dtype=np.float)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        words = tokenize_lower(sentence)[:word_max_count]\n",
    "        word_matrix[i, -len(words):] = [vocabulary.get_word_index(word) for word in words]\n",
    "        char_vectors = []\n",
    "        for word in tokenize(sentence):\n",
    "            char_indices = np.zeros(max_word_len)\n",
    "            word_char_indices = [CHAR_SET.index(ch) if ch in CHAR_SET else len(CHAR_SET) for ch in word]\n",
    "            char_indices[-min(len(word), max_word_len):] = word_char_indices[:max_word_len]\n",
    "            char_vectors.append(char_indices)\n",
    "        char_matrix[i, -len(tokenize(sentence)):] = char_vectors\n",
    "        forms = predictor.predict_sentence_tags(tokenize(sentence))\n",
    "        tags = [form.pos + \"#\" + form.tag for form in forms]\n",
    "        grammemes_matrix[i, -len(tokenize(sentence)):] = [grammeme_vectorizer.get_vector(tag) for tag in tags]\n",
    "    return word_matrix, char_matrix, grammemes_matrix\n",
    "\n",
    "def get_train_val_test_sets(x, y, x_test, vocabulary, word_max_count=50, max_word_len=30, val_part=0.1):\n",
    "    \n",
    "    word_matrix, char_matrix, grammemes_matrix = get_samples(x[\"text\"].tolist(), vocabulary, word_max_count, max_word_len)\n",
    "\n",
    "    n = x.shape[0]\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    perm = np.random.permutation(n)\n",
    "    idx_train = perm[:int(n*(1-val_part))]\n",
    "    idx_val = perm[int(n*(1-val_part)):]\n",
    "\n",
    "    word_matrix_train = word_matrix[idx_train]\n",
    "    char_matrix_train = char_matrix[idx_train]\n",
    "    grammemes_matrix_train = grammemes_matrix[idx_train]\n",
    "    y_train = np.array(y, dtype='int32')[idx_train]\n",
    "\n",
    "    word_matrix_val = word_matrix[idx_val]\n",
    "    char_matrix_val = char_matrix[idx_val]\n",
    "    grammemes_matrix_val = grammemes_matrix[idx_val]\n",
    "    y_val = np.array(y, dtype='int32')[idx_val]\n",
    "\n",
    "    word_matrix_test, char_matrix_test, grammemes_matrix_test = \\\n",
    "        get_samples(x_test[\"text\"].tolist(), vocabulary, word_max_count, max_word_len)\n",
    "\n",
    "    return (word_matrix_train, char_matrix_train, grammemes_matrix_train, y_train), \\\n",
    "        (word_matrix_val, char_matrix_val, grammemes_matrix_val, y_val), \\\n",
    "        (word_matrix_test, char_matrix_test, grammemes_matrix_test)\n",
    "\n",
    "if not os.path.exists(\"data_train.pickle\"):\n",
    "    data_train, data_val, data_test = get_train_val_test_sets(train, [a+1 for a in train['answer'].tolist()], test, vocabulary)\n",
    "    with open(\"data_train.pickle\", \"wb\") as train_file:\n",
    "        pickle.dump(data_train, train_file)\n",
    "    with open(\"data_val.pickle\", \"wb\") as val_file:\n",
    "        pickle.dump(data_val, val_file)\n",
    "    with open(\"data_test.pickle\", \"wb\") as test_file:\n",
    "        pickle.dump(data_test, test_file)\n",
    "else:\n",
    "    with open(\"data_train.pickle\", \"rb\") as train_file:\n",
    "        data_train = pickle.load(train_file)\n",
    "    with open(\"data_val.pickle\", \"rb\") as val_file:\n",
    "        data_val = pickle.load(val_file)\n",
    "    with open(\"data_test.pickle\", \"rb\") as test_file:\n",
    "        data_test = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiRNN:\n",
    "    def __init__(self, rnn_units=128, dense_units=64, dropout=0.5, batch_size=32, embeddings_dimensions=300, \n",
    "                 char_embeddings_dimension=5, max_word_len=30):\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.rnn_units = rnn_units\n",
    "        self.dense_units = dense_units\n",
    "        self.embeddings_dimensions = embeddings_dimensions\n",
    "        self.char_embeddings_dimension = char_embeddings_dimension\n",
    "        self.max_word_len = max_word_len\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def build(self, weights, vocabulary_size):\n",
    "        word_index_input = Input(shape=(None,), dtype=\"int32\", name=\"word_index_input\")\n",
    "        word_embeddings = Embedding(vocabulary_size+1, self.embeddings_dimensions, weights=[weights,],\n",
    "                                    name=\"word_embeddings\", trainable=False)(word_index_input)\n",
    "        \n",
    "        grammemes_input = Input(shape=(None, grammeme_vectorizer.grammemes_count()), name='grammemes')\n",
    "        grammemes_layer = Dense(30, activation='relu')(grammemes_input)\n",
    "        \n",
    "        chars_input = Input(shape=(None, self.max_word_len), name='chars_input')\n",
    "        chars_layer = Embedding(len(CHAR_SET) + 1, self.char_embeddings_dimension, name='char_embeddings')(chars_input)\n",
    "        \n",
    "        chars_layer = TimeDistributed(Conv1D(5, 4, activation='relu'))(chars_layer)\n",
    "        chars_layer = TimeDistributed(Dropout(self.dropout))(chars_layer)\n",
    "        chars_layer = TimeDistributed(MaxPooling1D())(chars_layer)\n",
    "        \n",
    "        chars_layer = TimeDistributed(Conv1D(3, 3, activation='relu'))(chars_layer)\n",
    "        chars_layer = TimeDistributed(Dropout(self.dropout))(chars_layer)\n",
    "        chars_layer = TimeDistributed(MaxPooling1D())(chars_layer)\n",
    "        \n",
    "        chars_layer = TimeDistributed(Flatten())(chars_layer)\n",
    "        \n",
    "        layer = concatenate([word_embeddings, grammemes_layer], name=\"LSTM_input\")\n",
    "        layer = Bidirectional(LSTM(self.rnn_units // 2, dropout=self.dropout, recurrent_dropout=self.dropout, return_sequences=True))(layer)\n",
    "        layer = Bidirectional(LSTM(self.rnn_units // 2, dropout=self.dropout, recurrent_dropout=self.dropout))(layer)\n",
    "        layer = Dense(self.dense_units, activation='relu')(layer)\n",
    "        layer = Dropout(self.dropout)(layer)\n",
    "        \n",
    "        predictions = Dense(3, activation='softmax')(layer)\n",
    "        model = Model(inputs=[word_index_input, chars_input, grammemes_input], outputs=predictions)\n",
    "        \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "                                                                                          \n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, data_train, data_val, model_filename, enable_checkpoints=True):\n",
    "        word_matrix_train, char_matrix_train, grammemes_matrix_train, y_train = data_train\n",
    "        word_matrix_val, char_matrix_val, grammemes_matrix_val, y_val = data_val\n",
    "        \n",
    "        print(\"Train example:\")\n",
    "        print(word_matrix_train[0])\n",
    "        print(char_matrix_train[0])\n",
    "        print(grammemes_matrix_train[0])\n",
    "        print(y_train[0])\n",
    "        \n",
    "        # Callback to prevent overfitting.\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "        # Callback to save best only model.\n",
    "        if enable_checkpoints:\n",
    "            callbacks.append(ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True))\n",
    "            \n",
    "        cw = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "        self.model.fit([word_matrix_train, char_matrix_train, grammemes_matrix_train], y_train, \n",
    "                       validation_data=([word_matrix_val, char_matrix_val, grammemes_matrix_val], y_val),\n",
    "                       epochs=50,\n",
    "                       batch_size=self.batch_size,\n",
    "                       shuffle=True, \n",
    "                       callbacks=callbacks,\n",
    "                       class_weight=cw,\n",
    "                       verbose=1)\n",
    "\n",
    "    def load(self, filename: str) -> None:\n",
    "        self.model = load_model(filename)\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def predict(self, data_test):\n",
    "        word_matrix, char_matrix, grammemes_matrix = data_test\n",
    "        \n",
    "        print(\"Test example: \")\n",
    "        print(word_matrix[0])\n",
    "        print(char_matrix[0])\n",
    "        print(grammemes_matrix[0])\n",
    "        preds = self.model.predict([word_matrix, char_matrix, grammemes_matrix], \n",
    "                                   batch_size=self.batch_size, verbose=1)\n",
    "        test_answers = [np.argmax(pred)-1 for pred in preds]\n",
    "        test['answer'] = test_answers\n",
    "        get_submission(test_filename, test, \"submission.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_index_input (InputLayer)    (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "grammemes (InputLayer)           (None, None, 52)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word_embeddings (Embedding)      (None, None, 300)     6559500     word_index_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_53 (Dense)                 (None, None, 30)      1590        grammemes[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_input (Concatenate)         (None, None, 330)     0           word_embeddings[0][0]            \n",
      "                                                                   dense_53[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional) (None, None, 128)     202240      LSTM_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional) (None, 128)           98816       bidirectional_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_54 (Dense)                 (None, 64)            8256        bidirectional_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 64)            0           dense_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 3)             195         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,870,597\n",
      "Trainable params: 311,097\n",
      "Non-trainable params: 6,559,500\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train example:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0   24 2042  120  207   31   37   11\n",
      " 2043 1791  156  117   88]\n",
      "[[  0   0   0 ...,   0   0   0]\n",
      " [  0   0   0 ...,   0   0   0]\n",
      " [  0   0   0 ...,   0   0   0]\n",
      " ..., \n",
      " [  0   0   0 ...,  17  16   5]\n",
      " [  0   0   0 ...,  24  10  10]\n",
      " [  0   0   0 ...,   0   0 119]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rnn = SentiRNN()\n",
    "rnn.build(weights, vocabulary.size())\n",
    "rnn.train(data_train, data_val, MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "chars_input (InputLayer)         (None, None, 30)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "char_embeddings (Embedding)      (None, None, 30, 5)   625         chars_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "word_index_input (InputLayer)    (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "grammemes (InputLayer)           (None, None, 52)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, None, 150)     0           char_embeddings[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "word_embeddings (Embedding)      (None, None, 300)     6559500     word_index_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, None, 32)      1696        grammemes[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, None, 32)      4832        time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "GRU_input (Concatenate)          (None, None, 364)     0           word_embeddings[0][0]            \n",
      "                                                                   dense_24[0][0]                   \n",
      "                                                                   dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "gru_11 (GRU)                     (None, None, 128)     189312      GRU_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "gru_12 (GRU)                     (None, 128)           98688       gru_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 64)            8256        gru_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 64)            0           dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 3)             195         dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,863,104\n",
      "Trainable params: 303,604\n",
      "Non-trainable params: 6,559,500\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Test example: \n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45 21 50 12  1]\n",
      "[[ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ...,  1 15 12]\n",
      " [ 0  0  0 ..., 52 46 49]\n",
      " [ 0  0  0 ..., 87 84 78]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "19808/19811 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "rnn = SentiRNN()\n",
    "rnn.load(MODEL_FILENAME)\n",
    "rnn.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts    -  positive { tp: 0, tn: 3100, fp: 0, fn: 318 } negative { tp: 538, tn: 2075, fp: 559, fn: 246 }\r\n",
      "Precision -  { positive: NaN, negative: 0.49042844120328166 }\r\n",
      "Recall    -  { positive: 0, negative: 0.6862244897959183 }\r\n",
      "F         -  { positive: NaN, negative: 0.5720361509835195 }\r\n",
      "F_R       -  NaN\r\n"
     ]
    }
   ],
   "source": [
    "!cd twit-calc-win64/; node calc.js bank ../submission.xml ../etalon.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bow_nb(train_sentences, train_answers, test_sentences):\n",
    "    train_data, test_data = bow(train_sentences, test_sentences)\n",
    "    nb = MultinomialNB()\n",
    "    clf = GridSearchCV(estimator=nb, \n",
    "                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n",
    "                       scoring=\"neg_log_loss\", cv=5)\n",
    "    clf.fit(train_data, train_answers)\n",
    "    print(\"CV: {}\".format(clf.best_score_))\n",
    "    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n",
    "\n",
    "def run_boc_nb(train_sentences, train_answers, test_sentences):\n",
    "    train_data, test_data = bow(train_sentences, test_sentences, tokenizer=None, use_tfidf=True, analyzer='char')\n",
    "    nb = MultinomialNB()\n",
    "    clf = GridSearchCV(estimator=nb, \n",
    "                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n",
    "                       scoring=\"neg_log_loss\", cv=5)\n",
    "    clf.fit(train_data, train_answers)\n",
    "    print(\"CV: {}\".format(clf.best_score_))\n",
    "    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n",
    "\n",
    "def collect_additional_features(train, test):\n",
    "    train_df = train.copy()\n",
    "    test_df = test.copy()\n",
    "    rus_stopwords = set(nltk.corpus.stopwords.words(\"russian\"))\n",
    "    \n",
    "    train_df[\"words\"] =  train_df[\"text\"].apply(lambda text: text.split())\n",
    "    test_df[\"words\"] = test_df[\"text\"].apply(lambda text: text.split())\n",
    "    \n",
    "    train_df[\"num_words\"] = train_df[\"words\"].apply(lambda words: len(words))\n",
    "    test_df[\"num_words\"] = test_df[\"words\"].apply(lambda words: len(words))\n",
    "    \n",
    "    train_df[\"num_unique_words\"] = train_df[\"words\"].apply(lambda words: len(set(words)))\n",
    "    test_df[\"num_unique_words\"] = test_df[\"words\"].apply(lambda words: len(set(words)))\n",
    "    \n",
    "    train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda text: len(text))\n",
    "    test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda text: len(text))\n",
    "    \n",
    "    train_df[\"num_stopwords\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w in rus_stopwords]))\n",
    "    test_df[\"num_stopwords\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w in rus_stopwords]))\n",
    "    \n",
    "    train_df[\"num_punctuations\"] = train_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n",
    "    test_df[\"num_punctuations\"] =test_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n",
    "    \n",
    "    train_df[\"num_words_upper\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n",
    "    test_df[\"num_words_upper\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n",
    "    \n",
    "    train_df[\"num_words_title\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n",
    "    test_df[\"num_words_title\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n",
    "    \n",
    "    train_df[\"mean_word_len\"] = train_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n",
    "    test_df[\"mean_word_len\"] = test_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n",
    "    \n",
    "#     pred_train, pred_test = run_bow_nb(train_df[\"text\"].tolist(), train_df[\"answer\"].tolist(), test_df[\"text\"].tolist())\n",
    "#     train_df[\"nb_count_neg\"] = pred_train[:,0]\n",
    "#     train_df[\"nb_count_neu\"] = pred_train[:,1]\n",
    "#     train_df[\"nb_count_pos\"] = pred_train[:,2]\n",
    "#     test_df[\"nb_count_neg\"] = pred_test[:,0]\n",
    "#     test_df[\"nb_count_neu\"] = pred_test[:,1]\n",
    "#     test_df[\"nb_count_pos\"] = pred_test[:,2]\n",
    "    \n",
    "#     pred_train, pred_test = run_boc_nb(train_df[\"text\"].tolist(), train_df[\"answer\"].tolist(), test_df[\"text\"].tolist())\n",
    "#     train_df[\"nb_count_chars_neg\"] = pred_train[:,0]\n",
    "#     train_df[\"nb_count_chars_neu\"] = pred_train[:,1]\n",
    "#     train_df[\"nb_count_chars_pos\"] = pred_train[:,2]\n",
    "#     test_df[\"nb_count_chars_neg\"] = pred_test[:,0]\n",
    "#     test_df[\"nb_count_chars_neu\"] = pred_test[:,1]\n",
    "#     test_df[\"nb_count_chars_pos\"] = pred_test[:,2]\n",
    "    \n",
    "    train_df.drop([\"text\", \"answer\", \"company\", \"sample_id\", \"words\"], axis=1, inplace=True)\n",
    "    test_df.drop([\"text\", \"answer\", \"company\", \"sample_id\", \"words\"], axis=1, inplace=True)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_df = scaler.fit_transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "    return csr_matrix(train_df), csr_matrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts    -  positive { tp: 124, tn: 2969, fp: 131, fn: 194 } negative { tp: 392, tn: 2307, fp: 327, fn: 392 }\r\n",
      "Precision -  { positive: 0.48627450980392156, negative: 0.545201668984701 }\r\n",
      "Recall    -  { positive: 0.389937106918239, negative: 0.5 }\r\n",
      "F         -  { positive: 0.4328097731239093, negative: 0.5216234198270127 }\r\n",
      "F_R       -  0.477216596475461\r\n"
     ]
    }
   ],
   "source": [
    "train_additional_features, test_additional_features = collect_additional_features(train, test)\n",
    "train_data, test_data = bow(train['text'].tolist(), test['text'].tolist(), stem=True, bow_ngrams=(1,2))\n",
    "boc_train_data, boc_test_data = bow(train['text'].tolist(), test['text'].tolist(), analyzer='char', tokenizer=None, bow_ngrams=(1,2))\n",
    "train_data = hstack([train_data, boc_train_data, train_additional_features])\n",
    "test_data = hstack([test_data, boc_test_data, test_additional_features])\n",
    "train_answer = train['answer'].tolist()\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_data, train_answer)\n",
    "\n",
    "test['answer'] = clf.predict(test_data)\n",
    "get_submission(test_filename, test, \"submission.xml\")\n",
    "!cd twit-calc-win64/; node calc.js bank ../submission.xml ../etalon.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
